{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torchvision.transforms import Compose, CenterCrop, Normalize, ToTensor\n",
    "from pytorch_wavelets import DWT1DForward, DWT1DInverse  # or simply DWT1D, IDWT1D\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_size = 64\n",
    "Noise = \"-4_db\"\n",
    "\n",
    "def Normalization(data):\n",
    "    \n",
    "    data_mean = data.mean()\n",
    "    data_std = data.std()\n",
    "    \n",
    "    data = data - data_mean\n",
    "    data = data / data_std\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root  \n",
    "        if not os.path.exists(self.root):\n",
    "            raise Exception(\"[!] {} not exists.\".format(root))\n",
    "        \n",
    "        #sort file names\n",
    "        self.input_paths = sorted(glob(os.path.join(self.root, '{}/*_train.npy'.format(\"IB_data/noise_data/\"+Noise+\"/train_data\"))))\n",
    "        self.label_paths = sorted(glob(os.path.join(self.root, '{}/*_lab.npy'.format(\"IB_data/noise_data/\"+Noise+\"/train_lab\"))))\n",
    "        self.name = os.path.basename(root)\n",
    "        \n",
    "        #print(self.input_paths)\n",
    "        #print(self.label_paths)\n",
    "        \n",
    "        if len(self.input_paths) == 0 or len(self.label_paths) == 0:\n",
    "            raise Exception(\"No signal/labels are found in {}\".format(self.root))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        Signal = np.load(self.input_paths[index])\n",
    "        Signal = Normalization(Signal)\n",
    "        \n",
    "        Label = np.load(self.label_paths[index])\n",
    "            \n",
    "        return Signal, Label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)\n",
    "    \n",
    "class Dataset_test(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        \n",
    "        if not os.path.exists(self.root):\n",
    "            raise Exception(\"[!] {} not exists.\".format(root))\n",
    "        \n",
    "        #sort file names\n",
    "        self.input_paths = sorted(glob(os.path.join(self.root, '{}/*_test.npy'.format(\"IB_data/noise_data/\"+Noise+\"/test_data\"))))\n",
    "        self.label_paths = sorted(glob(os.path.join(self.root, '{}/*_lab.npy'.format(\"IB_data/noise_data/\"+Noise+\"/test_lab\"))))\n",
    "        self.name = os.path.basename(root)\n",
    "        \n",
    "        if len(self.input_paths) == 0 or len(self.label_paths) == 0:\n",
    "            raise Exception(\"No sinagl/labels are found in {}\".format(self.root))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        Signal = np.load(self.input_paths[index])\n",
    "        Signal = Normalization(Signal)\n",
    "        \n",
    "        Label = np.load(self.label_paths[index])\n",
    "        \n",
    "        return Signal, Label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loader(dataset, batch_size, num_workers=0, shuffle = False, drop_last=False):\n",
    "\n",
    "    input_images = dataset\n",
    "    input_loader = torch.utils.data.DataLoader(dataset=input_images, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, drop_last=drop_last)\n",
    "\n",
    "    return input_loader\n",
    "\n",
    "train_loader = loader(Dataset('../../'), batch_size= batch_size, shuffle = True, drop_last=True)\n",
    "test_loader = loader(Dataset_test('../../'), batch_size= test_size, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 1\n",
    "numf =12\n",
    "\n",
    "class SConv_1D(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch, kernel, pad):\n",
    "        super(SConv_1D, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel, padding=pad),\n",
    "            nn.GroupNorm(6, out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WaveCNN_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaveCNN_1D, self).__init__()\n",
    "        \n",
    "        self.SConv1 = SConv_1D(input, numf, 5, 2)\n",
    "        self.DWT1= DWT1DForward(J=3, wave='db12').cuda()\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "\n",
    "        \n",
    "        self.SConv2 = SConv_1D(numf, numf*2, 5, 2)        \n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.DWT2= DWT1DForward(J=3, wave='db12').cuda() \n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.SConv3 = SConv_1D(numf*2, numf*4, 5, 2)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.DWT3= DWT1DForward(J=3, wave='db12').cuda()       \n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        \n",
    "\n",
    "        self.SConv4 = SConv_1D(numf*4, numf*8, 5, 2)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.DWT4= DWT1DForward(J=3, wave='db12').cuda()  \n",
    "        self.dropout4 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.SConv5 = SConv_1D(numf*8, numf*16, 5, 2) \n",
    "        self.pool5 = nn.MaxPool1d(2)\n",
    "        self.DWT5= DWT1DForward(J=3, wave='db12').cuda()  \n",
    "        self.dropout5 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.SConv6 = SConv_1D(numf*16, numf*32, 5, 2)              \n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d((1))\n",
    "        self.fc = nn.Linear(numf*32, 7)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        output = self.SConv1(input)\n",
    "        DMT_yl1,DMT_yh1 = self.DWT1(output)\n",
    "        output = torch.cat([DMT_yl1,DMT_yh1[2],DMT_yh1[1],DMT_yh1[0]], dim=2)\n",
    "        output = self.dropout1(output)\n",
    "        \n",
    "        output = self.SConv2(output)\n",
    "        output = self.pool2(output)\n",
    "        DMT_yl2,DMT_yh2 = self.DWT2(output)\n",
    "        output = torch.cat([DMT_yl2,DMT_yh2[2],DMT_yh2[1],DMT_yh2[0]], dim=2)\n",
    "        output = self.dropout2(output)\n",
    "        \n",
    "        output = self.SConv3(output)\n",
    "        output = self.pool3(output)        \n",
    "        DMT_yl3,DMT_yh3 = self.DWT3(output)\n",
    "        output = torch.cat([DMT_yl3,DMT_yh3[2],DMT_yh3[1],DMT_yh3[0]], dim=2)\n",
    "        output = self.dropout3(output)\n",
    "        \n",
    "        output = self.SConv4(output)\n",
    "        output = self.pool4(output)\n",
    "        DMT_yl4,DMT_yh4 = self.DWT4(output)\n",
    "        output = torch.cat([DMT_yl4,DMT_yh4[2],DMT_yh4[1],DMT_yh4[0]], dim=2)         \n",
    "        output = self.dropout4(output)\n",
    "        \n",
    "        output = self.SConv5(output)\n",
    "        output = self.pool5(output)\n",
    "        DMT_yl5,DMT_yh5 = self.DWT5(output)\n",
    "        output = torch.cat([DMT_yl5,DMT_yh5[2],DMT_yh5[1],DMT_yh5[0]], dim=2)    \n",
    "        output = self.dropout5(output)\n",
    "        \n",
    "        output = self.SConv6(output)             \n",
    "        #print(output.shape)    \n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WaveCNN_1D(\n",
      "  (SConv1): SConv_1D(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv1d(1, 12, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): GroupNorm(6, 12, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (DWT1): DWT1DForward()\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (SConv2): SConv_1D(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv1d(12, 24, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): GroupNorm(6, 24, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (DWT2): DWT1DForward()\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (SConv3): SConv_1D(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv1d(24, 48, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): GroupNorm(6, 48, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (DWT3): DWT1DForward()\n",
      "  (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  (SConv4): SConv_1D(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv1d(48, 96, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): GroupNorm(6, 96, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (DWT4): DWT1DForward()\n",
      "  (dropout4): Dropout(p=0.1, inplace=False)\n",
      "  (SConv5): SConv_1D(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv1d(96, 192, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): GroupNorm(6, 192, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (DWT5): DWT1DForward()\n",
      "  (dropout5): Dropout(p=0.1, inplace=False)\n",
      "  (SConv6): SConv_1D(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): GroupNorm(6, 384, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=384, out_features=7, bias=True)\n",
      ")\n",
      "# Model parameters: 496063\n"
     ]
    }
   ],
   "source": [
    "Model = WaveCNN_1D().to(device)\n",
    "\n",
    "print(Model)\n",
    "\n",
    "print('# Model parameters:', sum(param.numel() for param in Model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "Optimizer = torch.optim.Adam(Model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, train_loss:1.94776, train_acc:0.14592, test_loss:1.94103, test_acc:0.14339\n",
      "-----------------------------------------------\n",
      "Epoch:1, train_loss:1.92504, train_acc:0.18211, test_loss:1.86377, test_acc:0.23410\n",
      "-----------------------------------------------\n",
      "Epoch:2, train_loss:1.80445, train_acc:0.27962, test_loss:1.72228, test_acc:0.32757\n",
      "-----------------------------------------------\n",
      "Epoch:3, train_loss:1.69883, train_acc:0.33342, test_loss:1.69518, test_acc:0.31610\n",
      "-----------------------------------------------\n",
      "Epoch:4, train_loss:1.63090, train_acc:0.36698, test_loss:1.59889, test_acc:0.36200\n",
      "-----------------------------------------------\n",
      "Epoch:5, train_loss:1.55792, train_acc:0.39303, test_loss:1.49745, test_acc:0.40155\n",
      "-----------------------------------------------\n",
      "Epoch:6, train_loss:1.49827, train_acc:0.41753, test_loss:1.44530, test_acc:0.41247\n",
      "-----------------------------------------------\n",
      "Epoch:7, train_loss:1.43818, train_acc:0.43904, test_loss:1.37542, test_acc:0.44538\n",
      "-----------------------------------------------\n",
      "Epoch:8, train_loss:1.38384, train_acc:0.46268, test_loss:1.36092, test_acc:0.46363\n",
      "-----------------------------------------------\n",
      "Epoch:9, train_loss:1.33236, train_acc:0.47994, test_loss:1.24154, test_acc:0.51092\n",
      "-----------------------------------------------\n",
      "Epoch:10, train_loss:1.29337, train_acc:0.50127, test_loss:1.31406, test_acc:0.49475\n",
      "-----------------------------------------------\n",
      "Epoch:11, train_loss:1.24810, train_acc:0.52418, test_loss:1.17876, test_acc:0.54204\n",
      "-----------------------------------------------\n",
      "Epoch:12, train_loss:1.20887, train_acc:0.53510, test_loss:1.17683, test_acc:0.54701\n",
      "-----------------------------------------------\n",
      "Epoch:13, train_loss:1.17855, train_acc:0.55113, test_loss:1.13465, test_acc:0.56596\n",
      "-----------------------------------------------\n",
      "Epoch:14, train_loss:1.14293, train_acc:0.56730, test_loss:1.08657, test_acc:0.58324\n",
      "-----------------------------------------------\n",
      "Epoch:15, train_loss:1.11510, train_acc:0.57659, test_loss:1.06001, test_acc:0.59513\n",
      "-----------------------------------------------\n",
      "Epoch:16, train_loss:1.08007, train_acc:0.58850, test_loss:1.04011, test_acc:0.60205\n",
      "-----------------------------------------------\n",
      "Epoch:17, train_loss:1.05109, train_acc:0.60199, test_loss:1.00050, test_acc:0.61269\n",
      "-----------------------------------------------\n",
      "Epoch:18, train_loss:1.02120, train_acc:0.61354, test_loss:0.94482, test_acc:0.63910\n",
      "-----------------------------------------------\n",
      "Epoch:19, train_loss:1.00404, train_acc:0.61852, test_loss:0.93845, test_acc:0.63980\n",
      "-----------------------------------------------\n",
      "Epoch:20, train_loss:0.97532, train_acc:0.63161, test_loss:0.92869, test_acc:0.63993\n",
      "-----------------------------------------------\n",
      "Epoch:21, train_loss:0.94991, train_acc:0.64126, test_loss:0.86830, test_acc:0.66787\n",
      "-----------------------------------------------\n",
      "Epoch:22, train_loss:0.92905, train_acc:0.65068, test_loss:0.86741, test_acc:0.66330\n",
      "-----------------------------------------------\n",
      "Epoch:23, train_loss:0.91104, train_acc:0.65747, test_loss:0.83921, test_acc:0.67630\n",
      "-----------------------------------------------\n",
      "Epoch:24, train_loss:0.88217, train_acc:0.66952, test_loss:0.82310, test_acc:0.68183\n",
      "-----------------------------------------------\n",
      "Epoch:25, train_loss:0.86796, train_acc:0.67341, test_loss:0.80639, test_acc:0.68791\n",
      "-----------------------------------------------\n",
      "Epoch:26, train_loss:0.84750, train_acc:0.68293, test_loss:0.80376, test_acc:0.69676\n",
      "-----------------------------------------------\n",
      "Epoch:27, train_loss:0.83519, train_acc:0.68619, test_loss:0.87219, test_acc:0.66330\n",
      "-----------------------------------------------\n",
      "Epoch:28, train_loss:0.81043, train_acc:0.69706, test_loss:0.77599, test_acc:0.69704\n",
      "-----------------------------------------------\n",
      "Epoch:29, train_loss:0.79495, train_acc:0.70000, test_loss:0.73835, test_acc:0.71792\n",
      "-----------------------------------------------\n",
      "Epoch:30, train_loss:0.77980, train_acc:0.70779, test_loss:0.77350, test_acc:0.70838\n",
      "-----------------------------------------------\n",
      "Epoch:31, train_loss:0.76210, train_acc:0.71178, test_loss:0.73463, test_acc:0.71792\n",
      "-----------------------------------------------\n",
      "Epoch:32, train_loss:0.74412, train_acc:0.71866, test_loss:0.69811, test_acc:0.73686\n",
      "-----------------------------------------------\n",
      "Epoch:33, train_loss:0.73135, train_acc:0.72668, test_loss:0.67819, test_acc:0.74419\n",
      "-----------------------------------------------\n",
      "Epoch:34, train_loss:0.72244, train_acc:0.72835, test_loss:0.67408, test_acc:0.74239\n",
      "-----------------------------------------------\n",
      "Epoch:35, train_loss:0.70593, train_acc:0.73619, test_loss:0.67558, test_acc:0.74336\n",
      "-----------------------------------------------\n",
      "Epoch:36, train_loss:0.69273, train_acc:0.73958, test_loss:0.65372, test_acc:0.74945\n",
      "-----------------------------------------------\n",
      "Epoch:37, train_loss:0.67772, train_acc:0.74543, test_loss:0.61176, test_acc:0.76867\n",
      "-----------------------------------------------\n",
      "Epoch:38, train_loss:0.66723, train_acc:0.74787, test_loss:0.67306, test_acc:0.74378\n",
      "-----------------------------------------------\n",
      "Epoch:39, train_loss:0.65448, train_acc:0.75557, test_loss:0.66202, test_acc:0.75028\n",
      "-----------------------------------------------\n",
      "Epoch:40, train_loss:0.64006, train_acc:0.76377, test_loss:0.61127, test_acc:0.76825\n",
      "-----------------------------------------------\n",
      "Epoch:41, train_loss:0.62958, train_acc:0.76440, test_loss:0.60432, test_acc:0.76908\n",
      "-----------------------------------------------\n",
      "Epoch:42, train_loss:0.61475, train_acc:0.77011, test_loss:0.58861, test_acc:0.78028\n",
      "-----------------------------------------------\n",
      "Epoch:43, train_loss:0.61449, train_acc:0.76703, test_loss:0.60979, test_acc:0.77641\n",
      "-----------------------------------------------\n",
      "Epoch:44, train_loss:0.59845, train_acc:0.77459, test_loss:0.61275, test_acc:0.76798\n",
      "-----------------------------------------------\n",
      "Epoch:45, train_loss:0.59061, train_acc:0.77899, test_loss:0.55700, test_acc:0.79024\n",
      "-----------------------------------------------\n",
      "Epoch:46, train_loss:0.57971, train_acc:0.78270, test_loss:0.60721, test_acc:0.77378\n",
      "-----------------------------------------------\n",
      "Epoch:47, train_loss:0.57564, train_acc:0.78256, test_loss:0.53021, test_acc:0.80061\n",
      "-----------------------------------------------\n",
      "Epoch:48, train_loss:0.56731, train_acc:0.78863, test_loss:0.51989, test_acc:0.80462\n",
      "-----------------------------------------------\n",
      "Epoch:49, train_loss:0.55382, train_acc:0.79198, test_loss:0.50798, test_acc:0.81056\n",
      "-----------------------------------------------\n",
      "Epoch:50, train_loss:0.54844, train_acc:0.79538, test_loss:0.54651, test_acc:0.79508\n",
      "-----------------------------------------------\n",
      "Epoch:51, train_loss:0.53742, train_acc:0.80172, test_loss:0.57369, test_acc:0.78374\n",
      "-----------------------------------------------\n",
      "Epoch:52, train_loss:0.52281, train_acc:0.80734, test_loss:0.50830, test_acc:0.81070\n",
      "-----------------------------------------------\n",
      "Epoch:53, train_loss:0.52688, train_acc:0.80308, test_loss:0.55770, test_acc:0.78830\n",
      "-----------------------------------------------\n",
      "Epoch:54, train_loss:0.51109, train_acc:0.80851, test_loss:0.56314, test_acc:0.79397\n",
      "-----------------------------------------------\n",
      "Epoch:55, train_loss:0.50125, train_acc:0.81413, test_loss:0.48163, test_acc:0.82439\n",
      "-----------------------------------------------\n",
      "Epoch:56, train_loss:0.49762, train_acc:0.81458, test_loss:0.48671, test_acc:0.82121\n",
      "-----------------------------------------------\n",
      "Epoch:57, train_loss:0.49246, train_acc:0.81639, test_loss:0.47228, test_acc:0.82287\n",
      "-----------------------------------------------\n",
      "Epoch:58, train_loss:0.48582, train_acc:0.82151, test_loss:0.48741, test_acc:0.82287\n",
      "-----------------------------------------------\n",
      "Epoch:59, train_loss:0.46775, train_acc:0.82754, test_loss:0.51852, test_acc:0.80448\n",
      "-----------------------------------------------\n",
      "Epoch:60, train_loss:0.47485, train_acc:0.82341, test_loss:0.48689, test_acc:0.82287\n",
      "-----------------------------------------------\n",
      "Epoch:61, train_loss:0.45976, train_acc:0.82745, test_loss:0.48631, test_acc:0.81900\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:62, train_loss:0.45419, train_acc:0.83265, test_loss:0.47939, test_acc:0.82052\n",
      "-----------------------------------------------\n",
      "Epoch:63, train_loss:0.45581, train_acc:0.83179, test_loss:0.48187, test_acc:0.82342\n",
      "-----------------------------------------------\n",
      "Epoch:64, train_loss:0.44816, train_acc:0.83211, test_loss:0.46507, test_acc:0.82951\n",
      "-----------------------------------------------\n",
      "Epoch:65, train_loss:0.44394, train_acc:0.83496, test_loss:0.45114, test_acc:0.83449\n",
      "-----------------------------------------------\n",
      "Epoch:66, train_loss:0.44106, train_acc:0.83727, test_loss:0.50876, test_acc:0.80960\n",
      "-----------------------------------------------\n",
      "Epoch:67, train_loss:0.43704, train_acc:0.83632, test_loss:0.47560, test_acc:0.82163\n",
      "-----------------------------------------------\n",
      "Epoch:68, train_loss:0.41992, train_acc:0.84484, test_loss:0.43443, test_acc:0.83905\n",
      "-----------------------------------------------\n",
      "Epoch:69, train_loss:0.42535, train_acc:0.84289, test_loss:0.44029, test_acc:0.83794\n",
      "-----------------------------------------------\n",
      "Epoch:70, train_loss:0.41925, train_acc:0.84547, test_loss:0.49448, test_acc:0.82135\n",
      "-----------------------------------------------\n",
      "Epoch:71, train_loss:0.40875, train_acc:0.84728, test_loss:0.45198, test_acc:0.83407\n",
      "-----------------------------------------------\n",
      "Epoch:72, train_loss:0.40602, train_acc:0.85127, test_loss:0.41806, test_acc:0.84541\n",
      "-----------------------------------------------\n",
      "Epoch:73, train_loss:0.39909, train_acc:0.85131, test_loss:0.43370, test_acc:0.83905\n",
      "-----------------------------------------------\n",
      "Epoch:74, train_loss:0.39173, train_acc:0.85716, test_loss:0.46022, test_acc:0.83089\n",
      "-----------------------------------------------\n",
      "Epoch:75, train_loss:0.39152, train_acc:0.85607, test_loss:0.42105, test_acc:0.84555\n",
      "-----------------------------------------------\n",
      "Epoch:76, train_loss:0.38786, train_acc:0.85521, test_loss:0.49153, test_acc:0.81886\n",
      "-----------------------------------------------\n",
      "Epoch:77, train_loss:0.37970, train_acc:0.86051, test_loss:0.45475, test_acc:0.83421\n",
      "-----------------------------------------------\n",
      "Epoch:78, train_loss:0.37335, train_acc:0.85770, test_loss:0.41781, test_acc:0.84804\n",
      "-----------------------------------------------\n",
      "Epoch:79, train_loss:0.37049, train_acc:0.86381, test_loss:0.43019, test_acc:0.84320\n",
      "-----------------------------------------------\n",
      "Epoch:80, train_loss:0.36444, train_acc:0.86345, test_loss:0.45228, test_acc:0.83435\n",
      "-----------------------------------------------\n",
      "Epoch:81, train_loss:0.36448, train_acc:0.86513, test_loss:0.42066, test_acc:0.84776\n",
      "-----------------------------------------------\n",
      "Epoch:82, train_loss:0.35541, train_acc:0.87020, test_loss:0.38818, test_acc:0.85924\n",
      "-----------------------------------------------\n",
      "Epoch:83, train_loss:0.36063, train_acc:0.86771, test_loss:0.40820, test_acc:0.85177\n",
      "-----------------------------------------------\n",
      "Epoch:84, train_loss:0.35136, train_acc:0.86984, test_loss:0.44158, test_acc:0.84444\n",
      "-----------------------------------------------\n",
      "Epoch:85, train_loss:0.35104, train_acc:0.87015, test_loss:0.40286, test_acc:0.85108\n",
      "-----------------------------------------------\n",
      "Epoch:86, train_loss:0.34104, train_acc:0.87337, test_loss:0.38402, test_acc:0.85882\n",
      "-----------------------------------------------\n",
      "Epoch:87, train_loss:0.34588, train_acc:0.87129, test_loss:0.42297, test_acc:0.85053\n",
      "-----------------------------------------------\n",
      "Epoch:88, train_loss:0.33268, train_acc:0.87649, test_loss:0.40672, test_acc:0.85495\n",
      "-----------------------------------------------\n",
      "Epoch:89, train_loss:0.33040, train_acc:0.87541, test_loss:0.44434, test_acc:0.84154\n",
      "-----------------------------------------------\n",
      "Epoch:90, train_loss:0.33152, train_acc:0.87754, test_loss:0.43118, test_acc:0.84043\n",
      "-----------------------------------------------\n",
      "Epoch:91, train_loss:0.31813, train_acc:0.88093, test_loss:0.39047, test_acc:0.85633\n",
      "-----------------------------------------------\n",
      "Epoch:92, train_loss:0.32069, train_acc:0.88116, test_loss:0.38482, test_acc:0.86048\n",
      "-----------------------------------------------\n",
      "Epoch:93, train_loss:0.32109, train_acc:0.88157, test_loss:0.39239, test_acc:0.85896\n",
      "-----------------------------------------------\n",
      "Epoch:94, train_loss:0.31954, train_acc:0.87998, test_loss:0.40859, test_acc:0.85606\n",
      "-----------------------------------------------\n",
      "Epoch:95, train_loss:0.31590, train_acc:0.88179, test_loss:0.37961, test_acc:0.86283\n",
      "-----------------------------------------------\n",
      "Epoch:96, train_loss:0.31128, train_acc:0.88673, test_loss:0.37591, test_acc:0.86491\n",
      "-----------------------------------------------\n",
      "Epoch:97, train_loss:0.30689, train_acc:0.88764, test_loss:0.40968, test_acc:0.85205\n",
      "-----------------------------------------------\n",
      "Epoch:98, train_loss:0.30862, train_acc:0.88546, test_loss:0.40520, test_acc:0.85523\n",
      "-----------------------------------------------\n",
      "Epoch:99, train_loss:0.29847, train_acc:0.89017, test_loss:0.43273, test_acc:0.84513\n",
      "-----------------------------------------------\n",
      "Epoch:100, train_loss:0.29922, train_acc:0.88804, test_loss:0.39102, test_acc:0.85882\n",
      "-----------------------------------------------\n",
      "Epoch:101, train_loss:0.29341, train_acc:0.89198, test_loss:0.36850, test_acc:0.86629\n",
      "-----------------------------------------------\n",
      "Epoch:102, train_loss:0.29212, train_acc:0.89144, test_loss:0.38651, test_acc:0.86380\n",
      "-----------------------------------------------\n",
      "Epoch:103, train_loss:0.28882, train_acc:0.89053, test_loss:0.38009, test_acc:0.86726\n",
      "-----------------------------------------------\n",
      "Epoch:104, train_loss:0.29438, train_acc:0.89081, test_loss:0.37002, test_acc:0.86878\n",
      "-----------------------------------------------\n",
      "Epoch:105, train_loss:0.28480, train_acc:0.89284, test_loss:0.36808, test_acc:0.87016\n",
      "-----------------------------------------------\n",
      "Epoch:106, train_loss:0.28080, train_acc:0.89597, test_loss:0.40854, test_acc:0.85689\n",
      "-----------------------------------------------\n",
      "Epoch:107, train_loss:0.28172, train_acc:0.89520, test_loss:0.41106, test_acc:0.85689\n",
      "-----------------------------------------------\n",
      "Epoch:108, train_loss:0.27219, train_acc:0.90018, test_loss:0.38240, test_acc:0.86698\n",
      "-----------------------------------------------\n",
      "Epoch:109, train_loss:0.26323, train_acc:0.90530, test_loss:0.40334, test_acc:0.86186\n",
      "-----------------------------------------------\n",
      "Epoch:110, train_loss:0.27087, train_acc:0.89896, test_loss:0.37767, test_acc:0.86712\n",
      "-----------------------------------------------\n",
      "Epoch:111, train_loss:0.26558, train_acc:0.90208, test_loss:0.37985, test_acc:0.86657\n",
      "-----------------------------------------------\n",
      "Epoch:112, train_loss:0.26643, train_acc:0.90199, test_loss:0.40260, test_acc:0.85772\n",
      "-----------------------------------------------\n",
      "Epoch:113, train_loss:0.26426, train_acc:0.90163, test_loss:0.37870, test_acc:0.86878\n",
      "-----------------------------------------------\n",
      "Epoch:114, train_loss:0.26375, train_acc:0.90177, test_loss:0.34792, test_acc:0.87860\n",
      "-----------------------------------------------\n",
      "Epoch:115, train_loss:0.26076, train_acc:0.90358, test_loss:0.38561, test_acc:0.86504\n",
      "-----------------------------------------------\n",
      "Epoch:116, train_loss:0.26519, train_acc:0.90217, test_loss:0.39011, test_acc:0.86366\n",
      "-----------------------------------------------\n",
      "Epoch:117, train_loss:0.25054, train_acc:0.90738, test_loss:0.39512, test_acc:0.86449\n",
      "-----------------------------------------------\n",
      "Epoch:118, train_loss:0.25120, train_acc:0.90838, test_loss:0.35323, test_acc:0.87638\n",
      "-----------------------------------------------\n",
      "Epoch:119, train_loss:0.24845, train_acc:0.90806, test_loss:0.39548, test_acc:0.86283\n",
      "-----------------------------------------------\n",
      "-------保存第120 epoch的模型---------\n",
      "Epoch:120, train_loss:0.24906, train_acc:0.91069, test_loss:0.34146, test_acc:0.87956\n",
      "-----------------------------------------------\n",
      "-------保存第121 epoch的模型---------\n",
      "Epoch:121, train_loss:0.24807, train_acc:0.90865, test_loss:0.34194, test_acc:0.87970\n",
      "-----------------------------------------------\n",
      "-------保存第122 epoch的模型---------\n",
      "Epoch:122, train_loss:0.24624, train_acc:0.90987, test_loss:0.36653, test_acc:0.87334\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------保存第123 epoch的模型---------\n",
      "Epoch:123, train_loss:0.24235, train_acc:0.91150, test_loss:0.35896, test_acc:0.87735\n",
      "-----------------------------------------------\n",
      "-------保存第124 epoch的模型---------\n",
      "Epoch:124, train_loss:0.24036, train_acc:0.91209, test_loss:0.37752, test_acc:0.86809\n",
      "-----------------------------------------------\n",
      "-------保存第125 epoch的模型---------\n",
      "Epoch:125, train_loss:0.23541, train_acc:0.91205, test_loss:0.36555, test_acc:0.87431\n",
      "-----------------------------------------------\n",
      "-------保存第126 epoch的模型---------\n",
      "Epoch:126, train_loss:0.23116, train_acc:0.91418, test_loss:0.38695, test_acc:0.86988\n",
      "-----------------------------------------------\n",
      "-------保存第127 epoch的模型---------\n",
      "Epoch:127, train_loss:0.23774, train_acc:0.91123, test_loss:0.41272, test_acc:0.85882\n",
      "-----------------------------------------------\n",
      "-------保存第128 epoch的模型---------\n",
      "Epoch:128, train_loss:0.23704, train_acc:0.91282, test_loss:0.36313, test_acc:0.87334\n",
      "-----------------------------------------------\n",
      "-------保存第129 epoch的模型---------\n",
      "Epoch:129, train_loss:0.23386, train_acc:0.91490, test_loss:0.38832, test_acc:0.86961\n",
      "-----------------------------------------------\n",
      "-------保存第130 epoch的模型---------\n",
      "Epoch:130, train_loss:0.23055, train_acc:0.91449, test_loss:0.38692, test_acc:0.87265\n",
      "-----------------------------------------------\n",
      "-------保存第131 epoch的模型---------\n",
      "Epoch:131, train_loss:0.22855, train_acc:0.91540, test_loss:0.35273, test_acc:0.87846\n",
      "-----------------------------------------------\n",
      "-------保存第132 epoch的模型---------\n",
      "Epoch:132, train_loss:0.22334, train_acc:0.91748, test_loss:0.39172, test_acc:0.86905\n",
      "-----------------------------------------------\n",
      "-------保存第133 epoch的模型---------\n",
      "Epoch:133, train_loss:0.22378, train_acc:0.91744, test_loss:0.36574, test_acc:0.87583\n",
      "-----------------------------------------------\n",
      "-------保存第134 epoch的模型---------\n",
      "Epoch:134, train_loss:0.21709, train_acc:0.91934, test_loss:0.34650, test_acc:0.88164\n",
      "-----------------------------------------------\n",
      "-------保存第135 epoch的模型---------\n",
      "Epoch:135, train_loss:0.21801, train_acc:0.91898, test_loss:0.39215, test_acc:0.86670\n",
      "-----------------------------------------------\n",
      "-------保存第136 epoch的模型---------\n",
      "Epoch:136, train_loss:0.21014, train_acc:0.92296, test_loss:0.37181, test_acc:0.87666\n",
      "-----------------------------------------------\n",
      "-------保存第137 epoch的模型---------\n",
      "Epoch:137, train_loss:0.21416, train_acc:0.92197, test_loss:0.36440, test_acc:0.87403\n",
      "-----------------------------------------------\n",
      "-------保存第138 epoch的模型---------\n",
      "Epoch:138, train_loss:0.21230, train_acc:0.92260, test_loss:0.36580, test_acc:0.87721\n",
      "-----------------------------------------------\n",
      "-------保存第139 epoch的模型---------\n",
      "Epoch:139, train_loss:0.20711, train_acc:0.92455, test_loss:0.34314, test_acc:0.88247\n",
      "-----------------------------------------------\n",
      "-------保存第140 epoch的模型---------\n",
      "Epoch:140, train_loss:0.21361, train_acc:0.92165, test_loss:0.34291, test_acc:0.88343\n",
      "-----------------------------------------------\n",
      "-------保存第141 epoch的模型---------\n",
      "Epoch:141, train_loss:0.20796, train_acc:0.92328, test_loss:0.36536, test_acc:0.87500\n",
      "-----------------------------------------------\n",
      "-------保存第142 epoch的模型---------\n",
      "Epoch:142, train_loss:0.20939, train_acc:0.92346, test_loss:0.37404, test_acc:0.87279\n",
      "-----------------------------------------------\n",
      "-------保存第143 epoch的模型---------\n",
      "Epoch:143, train_loss:0.20577, train_acc:0.92613, test_loss:0.37925, test_acc:0.87154\n",
      "-----------------------------------------------\n",
      "-------保存第144 epoch的模型---------\n",
      "Epoch:144, train_loss:0.20732, train_acc:0.92396, test_loss:0.36495, test_acc:0.87860\n",
      "-----------------------------------------------\n",
      "-------保存第145 epoch的模型---------\n",
      "Epoch:145, train_loss:0.20050, train_acc:0.92631, test_loss:0.37408, test_acc:0.87320\n",
      "-----------------------------------------------\n",
      "-------保存第146 epoch的模型---------\n",
      "Epoch:146, train_loss:0.20122, train_acc:0.92717, test_loss:0.37667, test_acc:0.87569\n",
      "-----------------------------------------------\n",
      "-------保存第147 epoch的模型---------\n",
      "Epoch:147, train_loss:0.19732, train_acc:0.92763, test_loss:0.36961, test_acc:0.87376\n",
      "-----------------------------------------------\n",
      "-------保存第148 epoch的模型---------\n",
      "Epoch:148, train_loss:0.19672, train_acc:0.92736, test_loss:0.36734, test_acc:0.87970\n",
      "-----------------------------------------------\n",
      "-------保存第149 epoch的模型---------\n",
      "Epoch:149, train_loss:0.19339, train_acc:0.92903, test_loss:0.36401, test_acc:0.87832\n",
      "-----------------------------------------------\n",
      "-------保存第150 epoch的模型---------\n",
      "Epoch:150, train_loss:0.19203, train_acc:0.93057, test_loss:0.36646, test_acc:0.87569\n",
      "-----------------------------------------------\n",
      "-------保存第151 epoch的模型---------\n",
      "Epoch:151, train_loss:0.19192, train_acc:0.92980, test_loss:0.36223, test_acc:0.87763\n",
      "-----------------------------------------------\n",
      "-------保存第152 epoch的模型---------\n",
      "Epoch:152, train_loss:0.19431, train_acc:0.92713, test_loss:0.35623, test_acc:0.88025\n",
      "-----------------------------------------------\n",
      "-------保存第153 epoch的模型---------\n",
      "Epoch:153, train_loss:0.18618, train_acc:0.93247, test_loss:0.35328, test_acc:0.88012\n",
      "-----------------------------------------------\n",
      "-------保存第154 epoch的模型---------\n",
      "Epoch:154, train_loss:0.18703, train_acc:0.93143, test_loss:0.39274, test_acc:0.87237\n",
      "-----------------------------------------------\n",
      "-------保存第155 epoch的模型---------\n",
      "Epoch:155, train_loss:0.18644, train_acc:0.93365, test_loss:0.40167, test_acc:0.86947\n",
      "-----------------------------------------------\n",
      "-------保存第156 epoch的模型---------\n",
      "Epoch:156, train_loss:0.18609, train_acc:0.93170, test_loss:0.37129, test_acc:0.88025\n",
      "-----------------------------------------------\n",
      "-------保存第157 epoch的模型---------\n",
      "Epoch:157, train_loss:0.18584, train_acc:0.93293, test_loss:0.38667, test_acc:0.87251\n",
      "-----------------------------------------------\n",
      "-------保存第158 epoch的模型---------\n",
      "Epoch:158, train_loss:0.18444, train_acc:0.93143, test_loss:0.38465, test_acc:0.87210\n",
      "-----------------------------------------------\n",
      "-------保存第159 epoch的模型---------\n",
      "Epoch:159, train_loss:0.18320, train_acc:0.93388, test_loss:0.36763, test_acc:0.87735\n",
      "-----------------------------------------------\n",
      "-------保存第160 epoch的模型---------\n",
      "Epoch:160, train_loss:0.18154, train_acc:0.93483, test_loss:0.36465, test_acc:0.88108\n",
      "-----------------------------------------------\n",
      "-------保存第161 epoch的模型---------\n",
      "Epoch:161, train_loss:0.17908, train_acc:0.93469, test_loss:0.36047, test_acc:0.88053\n",
      "-----------------------------------------------\n",
      "-------保存第162 epoch的模型---------\n",
      "Epoch:162, train_loss:0.17843, train_acc:0.93696, test_loss:0.37022, test_acc:0.87873\n",
      "-----------------------------------------------\n",
      "-------保存第163 epoch的模型---------\n",
      "Epoch:163, train_loss:0.17758, train_acc:0.93474, test_loss:0.39674, test_acc:0.87279\n",
      "-----------------------------------------------\n",
      "-------保存第164 epoch的模型---------\n",
      "Epoch:164, train_loss:0.17990, train_acc:0.93370, test_loss:0.37758, test_acc:0.87887\n",
      "-----------------------------------------------\n",
      "-------保存第165 epoch的模型---------\n",
      "Epoch:165, train_loss:0.18321, train_acc:0.93324, test_loss:0.39861, test_acc:0.87140\n",
      "-----------------------------------------------\n",
      "-------保存第166 epoch的模型---------\n",
      "Epoch:166, train_loss:0.17509, train_acc:0.93591, test_loss:0.37065, test_acc:0.88067\n",
      "-----------------------------------------------\n",
      "-------保存第167 epoch的模型---------\n",
      "Epoch:167, train_loss:0.17988, train_acc:0.93528, test_loss:0.37571, test_acc:0.87514\n",
      "-----------------------------------------------\n",
      "-------保存第168 epoch的模型---------\n",
      "Epoch:168, train_loss:0.17217, train_acc:0.93718, test_loss:0.38550, test_acc:0.87749\n",
      "-----------------------------------------------\n",
      "-------保存第169 epoch的模型---------\n",
      "Epoch:169, train_loss:0.17191, train_acc:0.93745, test_loss:0.39741, test_acc:0.87306\n",
      "-----------------------------------------------\n",
      "-------保存第170 epoch的模型---------\n",
      "Epoch:170, train_loss:0.17166, train_acc:0.93664, test_loss:0.37190, test_acc:0.87956\n",
      "-----------------------------------------------\n",
      "-------保存第171 epoch的模型---------\n",
      "Epoch:171, train_loss:0.17028, train_acc:0.93777, test_loss:0.36981, test_acc:0.88053\n",
      "-----------------------------------------------\n",
      "-------保存第172 epoch的模型---------\n",
      "Epoch:172, train_loss:0.16819, train_acc:0.93745, test_loss:0.38209, test_acc:0.87569\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------保存第173 epoch的模型---------\n",
      "Epoch:173, train_loss:0.16429, train_acc:0.94085, test_loss:0.37364, test_acc:0.87680\n",
      "-----------------------------------------------\n",
      "-------保存第174 epoch的模型---------\n",
      "Epoch:174, train_loss:0.16628, train_acc:0.94008, test_loss:0.38383, test_acc:0.87721\n",
      "-----------------------------------------------\n",
      "-------保存第175 epoch的模型---------\n",
      "Epoch:175, train_loss:0.16876, train_acc:0.93791, test_loss:0.36478, test_acc:0.88399\n",
      "-----------------------------------------------\n",
      "-------保存第176 epoch的模型---------\n",
      "Epoch:176, train_loss:0.16724, train_acc:0.93845, test_loss:0.34038, test_acc:0.88827\n",
      "-----------------------------------------------\n",
      "-------保存第177 epoch的模型---------\n",
      "Epoch:177, train_loss:0.16594, train_acc:0.93913, test_loss:0.36699, test_acc:0.88164\n",
      "-----------------------------------------------\n",
      "-------保存第178 epoch的模型---------\n",
      "Epoch:178, train_loss:0.16297, train_acc:0.93881, test_loss:0.34982, test_acc:0.88537\n",
      "-----------------------------------------------\n",
      "-------保存第179 epoch的模型---------\n",
      "Epoch:179, train_loss:0.16240, train_acc:0.94026, test_loss:0.35332, test_acc:0.88689\n",
      "-----------------------------------------------\n",
      "-------保存第180 epoch的模型---------\n",
      "Epoch:180, train_loss:0.16377, train_acc:0.94053, test_loss:0.38399, test_acc:0.88343\n",
      "-----------------------------------------------\n",
      "-------保存第181 epoch的模型---------\n",
      "Epoch:181, train_loss:0.15582, train_acc:0.94411, test_loss:0.38453, test_acc:0.87998\n",
      "-----------------------------------------------\n",
      "-------保存第182 epoch的模型---------\n",
      "Epoch:182, train_loss:0.15514, train_acc:0.94407, test_loss:0.37753, test_acc:0.87735\n",
      "-----------------------------------------------\n",
      "-------保存第183 epoch的模型---------\n",
      "Epoch:183, train_loss:0.16101, train_acc:0.94171, test_loss:0.38037, test_acc:0.87873\n",
      "-----------------------------------------------\n",
      "-------保存第184 epoch的模型---------\n",
      "Epoch:184, train_loss:0.16665, train_acc:0.93995, test_loss:0.36786, test_acc:0.88067\n",
      "-----------------------------------------------\n",
      "-------保存第185 epoch的模型---------\n",
      "Epoch:185, train_loss:0.15779, train_acc:0.94321, test_loss:0.35221, test_acc:0.88758\n",
      "-----------------------------------------------\n",
      "-------保存第186 epoch的模型---------\n",
      "Epoch:186, train_loss:0.16191, train_acc:0.93972, test_loss:0.41376, test_acc:0.87002\n",
      "-----------------------------------------------\n",
      "-------保存第187 epoch的模型---------\n",
      "Epoch:187, train_loss:0.15465, train_acc:0.94488, test_loss:0.35448, test_acc:0.88883\n",
      "-----------------------------------------------\n",
      "-------保存第188 epoch的模型---------\n",
      "Epoch:188, train_loss:0.15566, train_acc:0.94375, test_loss:0.38906, test_acc:0.87528\n",
      "-----------------------------------------------\n",
      "-------保存第189 epoch的模型---------\n",
      "Epoch:189, train_loss:0.14943, train_acc:0.94638, test_loss:0.37002, test_acc:0.87887\n",
      "-----------------------------------------------\n",
      "-------保存第190 epoch的模型---------\n",
      "Epoch:190, train_loss:0.15537, train_acc:0.94293, test_loss:0.36170, test_acc:0.88537\n",
      "-----------------------------------------------\n",
      "-------保存第191 epoch的模型---------\n",
      "Epoch:191, train_loss:0.14567, train_acc:0.94860, test_loss:0.41068, test_acc:0.87362\n",
      "-----------------------------------------------\n",
      "-------保存第192 epoch的模型---------\n",
      "Epoch:192, train_loss:0.15189, train_acc:0.94669, test_loss:0.38210, test_acc:0.88178\n",
      "-----------------------------------------------\n",
      "-------保存第193 epoch的模型---------\n",
      "Epoch:193, train_loss:0.15099, train_acc:0.94588, test_loss:0.37427, test_acc:0.88288\n",
      "-----------------------------------------------\n",
      "-------保存第194 epoch的模型---------\n",
      "Epoch:194, train_loss:0.14500, train_acc:0.94778, test_loss:0.38130, test_acc:0.88205\n",
      "-----------------------------------------------\n",
      "-------保存第195 epoch的模型---------\n",
      "Epoch:195, train_loss:0.14846, train_acc:0.94660, test_loss:0.34485, test_acc:0.88509\n",
      "-----------------------------------------------\n",
      "-------保存第196 epoch的模型---------\n",
      "Epoch:196, train_loss:0.14610, train_acc:0.94706, test_loss:0.39694, test_acc:0.87873\n",
      "-----------------------------------------------\n",
      "-------保存第197 epoch的模型---------\n",
      "Epoch:197, train_loss:0.15063, train_acc:0.94579, test_loss:0.38437, test_acc:0.88095\n",
      "-----------------------------------------------\n",
      "-------保存第198 epoch的模型---------\n",
      "Epoch:198, train_loss:0.14074, train_acc:0.94841, test_loss:0.37360, test_acc:0.88482\n",
      "-----------------------------------------------\n",
      "-------保存第199 epoch的模型---------\n",
      "Epoch:199, train_loss:0.14673, train_acc:0.94728, test_loss:0.36985, test_acc:0.88095\n",
      "-----------------------------------------------\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc  = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    train_loss_sum, test_loss_sum, train_num, test_num, train_i, test_i = 0.0, 0.0, 0, 0, 0, 0\n",
    "    train_acc_sum,test_acc_sum = 0, 0\n",
    "    TEST_acc_sum = 0\n",
    "    \n",
    "    for i_1, train_data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = train_data\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.long()\n",
    "        labels = labels.squeeze(1)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        Model.train()\n",
    "        pre_labs = Model(inputs)\n",
    "\n",
    "        Loss = Criterion(pre_labs, labels)\n",
    "\n",
    "        Optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        Optimizer.step()\n",
    "        \n",
    "        train_loss_sum += Loss.item()\n",
    "        train_acc_sum += (pre_labs.argmax(dim=1) == labels).sum().item()\n",
    "        train_num += labels.shape[0]\n",
    "        train_i += 1\n",
    "\n",
    "    for i_2, data in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            test_data, test_lab = data  \n",
    "            test_data = test_data.unsqueeze(1)\n",
    "            test_data = test_data.type(torch.FloatTensor)\n",
    "            test_data = test_data.to(device)\n",
    "            test_lab = test_lab.long()\n",
    "            test_lab = test_lab.squeeze(1)\n",
    "            test_lab = test_lab.to(device)\n",
    "\n",
    "            Model.eval()\n",
    "            pre_test = Model(test_data)\n",
    "            t_loss = Criterion(pre_test, test_lab)\n",
    "\n",
    "            test_loss_sum += t_loss    \n",
    "            test_acc_sum += (pre_test.argmax(dim=1) == test_lab).sum().item()\n",
    "            test_num += test_lab.shape[0]                    \n",
    "            test_i += 1\n",
    "            \n",
    "            #_, Pred = torch.max(pre_test.data, 1)\n",
    "            #TEST_acc_sum += torch.sum(Pred == test_lab)\n",
    "           \n",
    "    if epoch >= 120:\n",
    "        torch.save(Model, \"model/4/CNN_%d.pkl\"% epoch)\n",
    "        print(\"-------保存第%d epoch的模型---------\"% epoch)\n",
    "       \n",
    "    Train_Loss = train_loss_sum/train_i\n",
    "    Test_Loss = test_loss_sum/test_i\n",
    "    Train_ACC = train_acc_sum/train_num\n",
    "    Test_ACC = test_acc_sum/test_num\n",
    "\n",
    "    print('Epoch:%d, train_loss:%.5f, train_acc:%.5f, test_loss:%.5f, test_acc:%.5f' % \n",
    "          (epoch, Train_Loss, Train_ACC, Test_Loss, Test_ACC))\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "    train_loss.append(Train_Loss)            \n",
    "    train_acc.append(Train_ACC) \n",
    "    test_loss.append(Test_Loss) \n",
    "    test_acc.append(Test_ACC)       \n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
